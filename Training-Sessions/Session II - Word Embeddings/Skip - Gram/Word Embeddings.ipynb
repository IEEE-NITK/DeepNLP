{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import hdf5storage\n",
    "\n",
    "f = hdf5storage.loadmat('data.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = f[\"data\"]\n",
    "train = data[\"trainData\"][0][0].transpose() - 1\n",
    "trainX = train[:,:3]\n",
    "targetX = train[:,3]\n",
    "\n",
    "valid = data[\"validData\"][0][0].transpose() - 1\n",
    "validX = valid[:,:3]\n",
    "validY = valid[:,3]\n",
    "\n",
    "test = data[\"testData\"][0][0].transpose() - 1\n",
    "testX = test[:,:3]\n",
    "testY = test[:,3]\n",
    "\n",
    "vocab = data[\"vocab\"][0][0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 27, 183, 182, ..., 222, 151,  90],\n",
       "       [ 25,  43,  31, ...,  89, 197,  31],\n",
       "       [ 89, 248,  75, ...,  95, 132, 222],\n",
       "       [143, 116, 121, ..., 203, 143, 199]], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"trainData\"][0][0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array(['going'], \n",
       "      dtype='<U5'),\n",
       "       array(['to'], \n",
       "      dtype='<U2'),\n",
       "       array(['be'], \n",
       "      dtype='<U2'), array(['.'], \n",
       "      dtype='<U1')], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[[27,25,89,143]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all' 'set' 'just' 'show' 'being' 'money' 'over' 'both' 'years' 'four'\n",
      " 'through' 'during' 'go' 'still' 'children' 'before' 'police' 'office'\n",
      " 'million' 'also' 'less' 'had' ',' 'including' 'should' 'to' 'only' 'going'\n",
      " 'under' 'has' 'might' 'do' 'them' 'good' 'around' 'get' 'very' 'big' 'dr.'\n",
      " 'game' 'every' 'know' 'they' 'not' 'world' 'now' 'him' 'school' 'several'\n",
      " 'like' 'did' 'university' 'companies' 'these' 'she' 'team' 'found' 'where'\n",
      " 'right' 'says' 'people' 'house' 'national' 'some' 'back' 'see' 'street'\n",
      " 'are' 'year' 'home' 'best' 'out' 'even' 'what' 'said' 'for' 'federal'\n",
      " 'since' 'its' 'may' 'state' 'does' 'john' 'between' 'new' ';' 'three'\n",
      " 'public' '?' 'be' 'we' 'after' 'business' 'never' 'use' 'here' 'york'\n",
      " 'members' 'percent' 'put' 'group' 'come' 'by' '$' 'on' 'about' 'last'\n",
      " 'her' 'of' 'could' 'days' 'against' 'times' 'women' 'place' 'think'\n",
      " 'first' 'among' 'own' 'family' 'into' 'each' 'one' 'down' 'because' 'long'\n",
      " 'another' 'such' 'old' 'next' 'your' 'market' 'second' 'city' 'little'\n",
      " 'from' 'would' 'few' 'west' 'there' 'political' 'two' 'been' '.' 'their'\n",
      " 'much' 'music' 'too' 'way' 'white' ':' 'was' 'war' 'today' 'more' 'ago'\n",
      " 'life' 'that' 'season' 'company' '-' 'but' 'part' 'court' 'former'\n",
      " 'general' 'with' 'than' 'those' 'he' 'me' 'high' 'made' 'this' 'work' 'up'\n",
      " 'us' 'until' 'will' 'ms.' 'while' 'officials' 'can' 'were' 'country' 'my'\n",
      " 'called' 'and' 'program' 'have' 'then' 'is' 'it' 'an' 'states' 'case'\n",
      " 'say' 'his' 'at' 'want' 'in' 'any' 'as' 'if' 'united' 'end' 'no' ')'\n",
      " 'make' 'government' 'when' 'american' 'same' 'how' 'mr.' 'other' 'take'\n",
      " 'which' 'department' '--' 'you' 'many' 'nt' 'day' 'week' 'play' 'used'\n",
      " \"'s\" 'though' 'our' 'who' 'yesterday' 'director' 'most' 'president' 'law'\n",
      " 'man' 'a' 'night' 'off' 'center' 'i' 'well' 'or' 'without' 'so' 'time'\n",
      " 'five' 'the' 'left']\n"
     ]
    }
   ],
   "source": [
    "#data[\"vocab\"][0][0][0].shape\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    vocab[i] = vocab[i][0]\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 27  25  89 143]\n",
      "[183  43 248 116]\n",
      "[182  31  75 121]\n",
      "[116 246 200 185]\n",
      "[222 189 248   5]\n"
     ]
    }
   ],
   "source": [
    "for row in train[:5,:]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_data(train):\n",
    "    batch = np.ndarray(shape=(len(train)*4), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(len(train)*4, 1), dtype=np.int32)\n",
    "    buffer = list()\n",
    "    labelbufffer = list()\n",
    "    for row in train:\n",
    "        buffer.append(row[1])\n",
    "        buffer.append(row[1])\n",
    "        labelbufffer.append([row[0]])\n",
    "        labelbufffer.append([row[2]])\n",
    "        buffer.append(row[2])\n",
    "        buffer.append(row[2])\n",
    "        labelbufffer.append([row[1]])\n",
    "        labelbufffer.append([row[3]])       \n",
    "        \n",
    "    batch = np.array(buffer)\n",
    "    labels = np.array(labelbufffer)\n",
    "    return batch,labels\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1490200\n"
     ]
    }
   ],
   "source": [
    "b,l = gen_data(train)\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7451.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape[0]/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "train_size = 1490200\n",
    "\n",
    "\n",
    "batch_size = 200\n",
    "num_batches = 7451\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "epochs = 10\n",
    "num_sampled = 25\n",
    "vocabulary_size = 250 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "# Weights and Biases for nce loss\n",
    "nce_weights = tf.Variable(\n",
    "  tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                      stddev=1.0 / math.sqrt(embedding_size)))\n",
    "\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# Placeholders for inputs\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "# Compute the NCE loss, using a sample of the negative labels each time.\n",
    "loss = tf.reduce_mean(\n",
    "  tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n",
    "                 num_sampled, vocabulary_size))\n",
    "\n",
    "# We use the SGD optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_batches(b,l, batch_size=200):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    num_batches = int(train_size/batch_size)\n",
    "    for i in range(num_batches):\n",
    "        input_buff = b[i*batch_size:(i+1)*batch_size]\n",
    "        label_buff = l[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        inputs.append(input_buff)\n",
    "        labels.append(label_buff)\n",
    "      \n",
    "    return zip(inputs,labels)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1  Loss :  2.16318\n",
      "Epoch # 2  Loss :  3.02129\n",
      "Epoch # 3  Loss :  1.87356\n",
      "Epoch # 4  Loss :  2.58635\n",
      "Epoch # 5  Loss :  2.14795\n",
      "Epoch # 6  Loss :  2.47761\n",
      "Epoch # 7  Loss :  2.07374\n",
      "Epoch # 8  Loss :  2.85126\n",
      "Epoch # 9  Loss :  2.13043\n",
      "Epoch # 10  Loss :  2.6385\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for inps, labls in gen_batches(b,l):\n",
    "            feed_dict = {train_inputs: inps, train_labels: labls}\n",
    "            _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        print(\"Epoch #\",epoch+1,\" Loss : \", cur_loss)\n",
    "        \n",
    "    word_embeddings  = embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07745709, -0.05804505,  0.00131782, ..., -0.31897098,\n",
       "         0.48136714, -0.01462711],\n",
       "       [-0.79447001, -0.71987045,  0.66330582, ..., -1.2404027 ,\n",
       "        -0.75644547,  0.52904773],\n",
       "       [ 0.71211201, -0.17814596,  0.17807025, ..., -0.04730324,\n",
       "        -0.04074759, -0.16970505],\n",
       "       ..., \n",
       "       [ 0.2184823 , -1.01309025,  0.04281931, ..., -1.19828022,\n",
       "        -0.08744483,  0.11709936],\n",
       "       [-0.14953709, -0.2999098 ,  0.08950499, ...,  0.17020635,\n",
       "         0.38466185,  0.04196095],\n",
       "       [ 0.53140539, -0.40258363, -0.48989901, ..., -0.60220033,\n",
       "         0.23682934,  0.25157347]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "\n",
    "    plt.savefig(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
